{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple tf.keras model\n",
    "Word embedding, LSTM, concatenation\n",
    "\n",
    "**GloVe weights info here: https://nlp.stanford.edu/projects/glove/**\n",
    "\n",
    "Score approximately 0.81 in current version - can easily be improved by playing around with the model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries needed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define functions for converting the attributes and text to decimal numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_test_attributes_to_onehot(df, value_list):\n",
    "    # Return a one-hit encoded numpy array for test data keyword and location (because that is easier)\n",
    "    one_hots = np.zeros((len(df), len(value_list)))\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if row['keyword'] != '':\n",
    "            try:\n",
    "                one_hots[index, value_list.index(row['keyword'])] = 1\n",
    "            except ValueError:\n",
    "#                print('could not find keyword: [' + str(row['keyword']) + '] at index [' + str(index) + ']')\n",
    "                continue\n",
    "\n",
    "        if row['keyword'] != '':\n",
    "            try:\n",
    "                one_hots[index, value_list.index(row['location'])] = 1\n",
    "            except ValueError:\n",
    "#                print('could not find location: [' + str(row['keyword']) + '] at index [' + str(index) + ']')\n",
    "                continue\n",
    "\n",
    "    return one_hots\n",
    "    \n",
    "\n",
    "def convert_attributes_to_onehot(df):\n",
    "    # Return a one-hit encoded dataframe for keyword and location\n",
    "    one_hot_keywords = pd.get_dummies(train_data['keyword'])\n",
    "    one_hot_locations = pd.get_dummies(train_data['location'])\n",
    "\n",
    "    return pd.concat([one_hot_keywords, one_hot_locations.reindex(one_hot_keywords.index)], axis=1)\n",
    "          \n",
    "    \n",
    "def vectorize_tweets(dataframe, df_glove):\n",
    "    # First create a blank matrix for our word vectors\n",
    "    vectorized_data = np.zeros((len(dataframe), MAX_TWEET_LENGTH, VECTORS_PER_WORD))\n",
    "    \n",
    "    # Now loop thorugh the words and add the corresponding vectors to the matrix\n",
    "    for index, row in dataframe.iterrows():\n",
    "        i = 0\n",
    "        # Remove special characters and convert text to lower case, then split on space\n",
    "        for word in re.sub(r'\\W+', ' ', row['text'].lower()).split(' '):\n",
    "            try:\n",
    "                vectorized_data[index, i, :] = df_glove.loc[word]\n",
    "            except KeyError:\n",
    "                # print(word + ' not found')\n",
    "                continue\n",
    "                \n",
    "            i += 1\n",
    "            \n",
    "        # Print a message for every 100 texts processed - just to check if we are still alive\n",
    "        if index % 100 == 99:\n",
    "            print('Processing text number ' + str(index + 1) + ' of ' + str(len(dataframe)))\n",
    "        \n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define function for creating the neural network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(lstm_shape, dense_shape):\n",
    "    dropout = 0.5\n",
    "    \n",
    "    # First part of the network for keyword and location\n",
    "    dense_input = tf.keras.layers.Input(shape=(dense_shape))\n",
    "    dense1 = tf.keras.layers.Dense(15)(dense_input)\n",
    "    \n",
    "    # Second part of the network for the text analysis\n",
    "    lstm_input = tf.keras.layers.Input(shape=(lstm_shape))\n",
    "    lstm1 = tf.keras.layers.LSTM(units=350, return_sequences=True)(lstm_input)\n",
    "    lstm2 = tf.keras.layers.Dropout(dropout)(lstm1)\n",
    "    lstm3 = tf.keras.layers.LSTM(units=50, return_sequences=True)(lstm2)\n",
    "    lstm4 = tf.keras.layers.Flatten()(lstm3)\n",
    "    lstm5 = tf.keras.layers.Dense(50)(lstm4)\n",
    "    \n",
    "    # Concatenate part for concatenating the two parts above\n",
    "    concat1 = tf.keras.layers.Concatenate()([dense1, lstm5])\n",
    "    concat2 = tf.keras.layers.Dropout(dropout)(concat1)\n",
    "    concat3 = tf.keras.layers.Dense(50)(concat2)\n",
    "    \n",
    "    # Output of model\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(concat3)\n",
    "    \n",
    "    # Compile the model\n",
    "    model = tf.keras.Model(inputs=[dense_input, lstm_input], outputs=outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "dropout = 0.5\n",
    "\n",
    "# First part of the network for keyword and location\n",
    "dense_input = tf.keras.layers.Input(shape=(3212))\n",
    "dense1 = tf.keras.layers.Dense(15)(dense_input)\n",
    "\n",
    "# Second part of the network for the text analysis\n",
    "lstm_input = tf.keras.layers.Input(shape=(20,20))\n",
    "lstm1 = tf.keras.layers.LSTM(units=350, return_sequences=True)(lstm_input)\n",
    "lstm2 = tf.keras.layers.Dropout(dropout)(lstm1)\n",
    "lstm3 = tf.keras.layers.LSTM(units=50, return_sequences=True)(lstm2)\n",
    "lstm4 = tf.keras.layers.Flatten()(lstm3)\n",
    "lstm5 = tf.keras.layers.Dense(50)(lstm4)\n",
    "\n",
    "# Concatenate part for concatenating the two parts above\n",
    "concat1 = tf.keras.layers.Concatenate()([dense1, lstm5])\n",
    "concat2 = tf.keras.layers.Dropout(dropout)(concat1)\n",
    "concat3 = tf.keras.layers.Dense(50)(concat2)\n",
    "\n",
    "# Output of model\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(concat3)\n",
    "\n",
    "# Compile the model\n",
    "model = tf.keras.Model(inputs=[dense_input, lstm_input], outputs=outputs)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
    "          loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "          metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constants needed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TWEET_LENGTH = 280 # Tweets can be a maximum of 280 characters\n",
    "VECTORS_PER_WORD = 50  # The number of individual vector values that represents a word\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data and convert it to something that can be represented with decimal numbers**\n",
    "This takes a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test and training data\n",
      "Loading GloVe file\n",
      "Processing text number 100 of 7613\n",
      "Processing text number 200 of 7613\n",
      "Processing text number 300 of 7613\n",
      "Processing text number 400 of 7613\n",
      "Processing text number 500 of 7613\n",
      "Processing text number 600 of 7613\n",
      "Processing text number 700 of 7613\n",
      "Processing text number 800 of 7613\n",
      "Processing text number 900 of 7613\n",
      "Processing text number 1000 of 7613\n",
      "Processing text number 1100 of 7613\n",
      "Processing text number 1200 of 7613\n",
      "Processing text number 1300 of 7613\n",
      "Processing text number 1400 of 7613\n",
      "Processing text number 1500 of 7613\n",
      "Processing text number 1600 of 7613\n",
      "Processing text number 1700 of 7613\n",
      "Processing text number 1800 of 7613\n",
      "Processing text number 1900 of 7613\n",
      "Processing text number 2000 of 7613\n",
      "Processing text number 2100 of 7613\n",
      "Processing text number 2200 of 7613\n",
      "Processing text number 2300 of 7613\n",
      "Processing text number 2400 of 7613\n",
      "Processing text number 2500 of 7613\n",
      "Processing text number 2600 of 7613\n",
      "Processing text number 2700 of 7613\n",
      "Processing text number 2800 of 7613\n",
      "Processing text number 2900 of 7613\n",
      "Processing text number 3000 of 7613\n",
      "Processing text number 3100 of 7613\n",
      "Processing text number 3200 of 7613\n",
      "Processing text number 3300 of 7613\n",
      "Processing text number 3400 of 7613\n",
      "Processing text number 3500 of 7613\n",
      "Processing text number 3600 of 7613\n",
      "Processing text number 3700 of 7613\n",
      "Processing text number 3800 of 7613\n",
      "Processing text number 3900 of 7613\n",
      "Processing text number 4000 of 7613\n",
      "Processing text number 4100 of 7613\n",
      "Processing text number 4200 of 7613\n",
      "Processing text number 4300 of 7613\n",
      "Processing text number 4400 of 7613\n",
      "Processing text number 4500 of 7613\n",
      "Processing text number 4600 of 7613\n",
      "Processing text number 4700 of 7613\n",
      "Processing text number 4800 of 7613\n",
      "Processing text number 4900 of 7613\n",
      "Processing text number 5000 of 7613\n",
      "Processing text number 5100 of 7613\n",
      "Processing text number 5200 of 7613\n",
      "Processing text number 5300 of 7613\n",
      "Processing text number 5400 of 7613\n",
      "Processing text number 5500 of 7613\n",
      "Processing text number 5600 of 7613\n",
      "Processing text number 5700 of 7613\n",
      "Processing text number 5800 of 7613\n",
      "Processing text number 5900 of 7613\n",
      "Processing text number 6000 of 7613\n",
      "Processing text number 6100 of 7613\n",
      "Processing text number 6200 of 7613\n",
      "Processing text number 6300 of 7613\n",
      "Processing text number 6400 of 7613\n",
      "Processing text number 6500 of 7613\n",
      "Processing text number 6600 of 7613\n",
      "Processing text number 6700 of 7613\n",
      "Processing text number 6800 of 7613\n",
      "Processing text number 6900 of 7613\n",
      "Processing text number 7000 of 7613\n",
      "Processing text number 7100 of 7613\n",
      "Processing text number 7200 of 7613\n",
      "Processing text number 7300 of 7613\n",
      "Processing text number 7400 of 7613\n",
      "Processing text number 7500 of 7613\n",
      "Processing text number 7600 of 7613\n",
      "Processing text number 100 of 3263\n",
      "Processing text number 200 of 3263\n",
      "Processing text number 300 of 3263\n",
      "Processing text number 400 of 3263\n",
      "Processing text number 500 of 3263\n",
      "Processing text number 600 of 3263\n",
      "Processing text number 700 of 3263\n",
      "Processing text number 800 of 3263\n",
      "Processing text number 900 of 3263\n",
      "Processing text number 1000 of 3263\n",
      "Processing text number 1100 of 3263\n",
      "Processing text number 1200 of 3263\n",
      "Processing text number 1300 of 3263\n",
      "Processing text number 1400 of 3263\n",
      "Processing text number 1500 of 3263\n",
      "Processing text number 1600 of 3263\n",
      "Processing text number 1700 of 3263\n",
      "Processing text number 1800 of 3263\n",
      "Processing text number 1900 of 3263\n",
      "Processing text number 2000 of 3263\n",
      "Processing text number 2100 of 3263\n",
      "Processing text number 2200 of 3263\n",
      "Processing text number 2300 of 3263\n",
      "Processing text number 2400 of 3263\n",
      "Processing text number 2500 of 3263\n",
      "Processing text number 2600 of 3263\n",
      "Processing text number 2700 of 3263\n",
      "Processing text number 2800 of 3263\n",
      "Processing text number 2900 of 3263\n",
      "Processing text number 3000 of 3263\n",
      "Processing text number 3100 of 3263\n",
      "Processing text number 3200 of 3263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.3466   ,  0.40689  , -0.079036 , ...,  0.31382  ,\n",
       "         -0.18248  ,  0.10831  ],\n",
       "        [ 0.04911  , -0.2102   , -0.26752  , ..., -0.17214  ,\n",
       "          0.7101   ,  0.0040847],\n",
       "        [ 0.96193  ,  0.012516 ,  0.21733  , ...,  0.14032  ,\n",
       "         -0.38468  , -0.38712  ],\n",
       "        ...,\n",
       "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "          0.       ,  0.       ],\n",
       "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "          0.       ,  0.       ],\n",
       "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "          0.       ,  0.       ]],\n",
       "\n",
       "       [[-0.0033744,  0.48159  , -0.38963  , ..., -0.92896  ,\n",
       "          0.4294   , -0.87843  ],\n",
       "        [ 0.50905  , -0.36805  ,  0.41275  , ..., -0.03356  ,\n",
       "          0.056012 , -0.43283  ],\n",
       "        [ 1.2142   ,  0.56772  ,  0.10257  , ..., -0.95912  ,\n",
       "          0.211    , -0.89709  ],\n",
       "        ...,\n",
       "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "          0.       ,  0.       ],\n",
       "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "          0.       ,  0.       ],\n",
       "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "          0.       ,  0.       ]],\n",
       "\n",
       "       [[ 0.19253  ,  0.10006  ,  0.063798 , ...,  0.081191 ,\n",
       "         -0.30485  , -0.30513  ],\n",
       "        [ 0.75149  , -0.090452 ,  0.50937  , ..., -0.42574  ,\n",
       "         -0.077817 , -0.32762  ],\n",
       "        [ 0.30223  , -0.037525 ,  0.064813 , ...,  0.45976  ,\n",
       "         -0.22192  ,  0.78108  ],\n",
       "        ...,\n",
       "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "          0.       ,  0.       ],\n",
       "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "          0.       ,  0.       ],\n",
       "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "          0.       ,  0.       ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.11436  ,  0.14762  ,  1.7862   , ...,  0.53374  ,\n",
       "          0.23909  ,  0.25272  ],\n",
       "        [-0.75656  ,  0.51181  ,  1.221    , ...,  0.038124 ,\n",
       "          0.2854   ,  0.049224 ],\n",
       "        [-1.1659   ,  0.11103  ,  0.68565  , ..., -0.09953  ,\n",
       "          1.2763   ,  0.34665  ],\n",
       "        ...,\n",
       "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "          0.       ,  0.       ],\n",
       "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "          0.       ,  0.       ],\n",
       "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "          0.       ,  0.       ]],\n",
       "\n",
       "       [[ 0.49725  , -1.1949   ,  0.37137  , ...,  0.12378  ,\n",
       "          0.85397  , -0.65035  ],\n",
       "        [ 1.2636   , -0.77308  , -0.11829  , ...,  1.1014   ,\n",
       "          0.73296  , -0.27085  ],\n",
       "        [ 0.38315  , -0.3561   , -0.1283   , ..., -0.4258   ,\n",
       "          0.13681  , -0.77731  ],\n",
       "        ...,\n",
       "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "          0.       ,  0.       ],\n",
       "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "          0.       ,  0.       ],\n",
       "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "          0.       ,  0.       ]],\n",
       "\n",
       "       [[ 0.418    ,  0.24968  , -0.41242  , ..., -0.18411  ,\n",
       "         -0.11514  , -0.78581  ],\n",
       "        [ 0.53201  ,  0.010601 ,  0.14717  , ...,  1.3226   ,\n",
       "          0.1169   ,  0.062825 ],\n",
       "        [ 0.87943  , -0.11176  ,  0.4338   , ...,  0.37183  ,\n",
       "         -0.71368  ,  0.30175  ],\n",
       "        ...,\n",
       "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "          0.       ,  0.       ],\n",
       "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "          0.       ,  0.       ],\n",
       "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "          0.       ,  0.       ]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start out by loading the train and test files into Pandas\n",
    "print('Loading test and training data')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "train_data = pd.read_csv('train.csv')\n",
    "\n",
    "# Next load the glove file \n",
    "print('Loading GloVe file')\n",
    "glove_data = pd.read_csv('glove.6B.50d.txt', sep=' ', index_col=0, header = None, quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "\n",
    "\n",
    "df_one_hot_attributes = convert_attributes_to_onehot(train_data)\n",
    "df_test_one_hot_attributes = convert_test_attributes_to_onehot(test_data, df_one_hot_attributes.columns.to_list())\n",
    "df_one_hot_attributes.head()\n",
    "# Now, vectorize the test and training data so that it can be understood by our model\n",
    "vectorized_training_data = vectorize_tweets(train_data, glove_data)\n",
    "vectorized_testing_data = vectorize_tweets(test_data, glove_data)\n",
    "vectorized_training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build the model, fit with data, make predictions and save to submit file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            [(None, 280, 50)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 280, 350)     561400      input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 280, 350)     0           lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 280, 50)      80200       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 3562)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 14000)        0           lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 15)           53445       input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 50)           700050      flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 65)           0           dense_12[0][0]                   \n",
      "                                                                 dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 65)           0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 50)           3300        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1)            51          dense_14[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,398,446\n",
      "Trainable params: 1,398,446\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/4\n",
      "118/118 [==============================] - 162s 1s/step - loss: 0.6804 - acc: 0.5790 - val_loss: 0.6241 - val_acc: 0.7143\n",
      "Epoch 2/4\n",
      "118/118 [==============================] - 170s 1s/step - loss: 0.5932 - acc: 0.7392 - val_loss: 0.3984 - val_acc: 0.8831\n",
      "Epoch 3/4\n",
      "118/118 [==============================] - 164s 1s/step - loss: 0.4974 - acc: 0.7798 - val_loss: 0.3435 - val_acc: 0.8961\n",
      "Epoch 4/4\n",
      "118/118 [==============================] - 164s 1s/step - loss: 0.4975 - acc: 0.7812 - val_loss: 0.3088 - val_acc: 0.9221\n",
      "Training complete. Commencing creation of submission file\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "# Build the model, show a summary and fit the model\n",
    "model = build_model([MAX_TWEET_LENGTH, VECTORS_PER_WORD], len(df_one_hot_attributes.columns))\n",
    "model.summary()\n",
    "\n",
    "# Save the epoch with the lowest validation loss\n",
    "checkpoint_save = tf.keras.callbacks.ModelCheckpoint('saved_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "# Use this to train on all training data - no validation\n",
    "#model.fit(x=[df_one_hot_attributes, vectorized_training_data], y=train_data['target'], batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose=1, shuffle=True, callbacks=[checkpoint_save])\n",
    "# Use this to split 0.9:0.1 between train and validation\n",
    "model.fit(x=[df_one_hot_attributes, vectorized_training_data], y=train_data['target'], batch_size=BATCH_SIZE,\n",
    "          epochs=4, verbose=1, shuffle=True, validation_split=0.01, callbacks=[checkpoint_save])\n",
    "\n",
    "# Reload the best model for predictions\n",
    "tf.keras.models.load_model('saved_model.h5')\n",
    "\n",
    "# Create the submission file\n",
    "print('Training complete. Commencing creation of submission file')\n",
    "df_submission = pd.DataFrame()\n",
    "df_submission['id'] = test_data['id']\n",
    "\n",
    "predictions = model.predict([df_test_one_hot_attributes, vectorized_testing_data])\n",
    "prediction_list = []\n",
    "for prediction in predictions:\n",
    "    if prediction < 0.5:\n",
    "        prediction_list += [0]\n",
    "    else:\n",
    "        prediction_list += [1]\n",
    "        \n",
    "df_submission['target'] = prediction_list\n",
    "\n",
    "df_submission.head(5)\n",
    "df_submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print('All done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorized_training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2acf6d710083>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorized_training_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorized_training_data' is not defined"
     ]
    }
   ],
   "source": [
    "vectorized_training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('saved_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict([df_test_one_hot_attributes, vectorized_testing_data])\n",
    "prediction_list = []\n",
    "for prediction in predictions:\n",
    "    if prediction < 0.5:\n",
    "        prediction_list += [0]\n",
    "    else:\n",
    "        prediction_list += [1]\n",
    "        \n",
    "df_submission['target'] = prediction_list\n",
    "\n",
    "df_submission.head(5)\n",
    "df_submission.to_csv('submission-other.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
